{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data.dataloader as dataloader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSJ():\n",
    "    \"\"\" Load the WSJ speech dataset\n",
    "        \n",
    "        Ensure WSJ_PATH is path to directory containing \n",
    "        all data files (.npy) provided on Kaggle.\n",
    "        \n",
    "        Example usage:\n",
    "            loader = WSJ()\n",
    "            trainX, trainY = loader.train\n",
    "            assert(trainX.shape[0] == 24590)\n",
    "            \n",
    "    \"\"\"\n",
    "  \n",
    "    def __init__(self, path):\n",
    "        self.dev_set = None\n",
    "        self.train_set = None\n",
    "        self.test_set = None\n",
    "        self.path  = path\n",
    "        \n",
    "    @property\n",
    "    def dev(self):\n",
    "        if self.dev_set is None:\n",
    "            self.dev_set = load_raw(self.path, 'dev')\n",
    "        return self.dev_set\n",
    "\n",
    "    @property\n",
    "    def train(self):\n",
    "        if self.train_set is None:\n",
    "            self.train_set = load_raw(self.path, 'train')\n",
    "        return self.train_set\n",
    "  \n",
    "    @property\n",
    "    def test(self):\n",
    "        if self.test_set is None:\n",
    "            self.test_set = (np.load(os.path.join(self.path, 'test.npy'), encoding='bytes'), None)\n",
    "        return self.test_set\n",
    "    \n",
    "def load_raw(path, name):\n",
    "    return (\n",
    "        np.load(os.path.join(path, '{}.npy'.format(name)), encoding='bytes'), \n",
    "        np.load(os.path.join(path, '{}_labels.npy'.format(name)), encoding='bytes')\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/home/borowis/s3\"\n",
    "wsj = WSJ(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 30\n",
    "bs = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1103,)\n",
      "(1103,)\n",
      "(482, 40)\n",
      "[[ -2.7760592  -10.653754    -9.3995695  ...   0.2363553   -0.5805931\n",
      "   -0.8171587 ]\n",
      " [ -2.2426343   -9.265765    -9.315787   ...  -0.26111507  -0.46208572\n",
      "   -0.9445448 ]\n",
      " [ -2.7435112   -6.7105646  -11.795384   ...  -0.6318717   -0.56550837\n",
      "   -1.3585529 ]\n",
      " ...\n",
      " [ -5.409962   -14.84733    -16.779129   ...  -3.2989292   -3.1955743\n",
      "   -3.436795  ]\n",
      " [ -5.069687   -13.877764   -18.607395   ...  -3.1886454   -3.2329483\n",
      "   -3.5929215 ]\n",
      " [ -5.1191835  -16.154802   -19.51888    ...  -1.9263964   -1.8058281\n",
      "   -2.1436906 ]]\n",
      "[108 109 110  45  46  47  24  25  26  57  58  59  59  69  69  69  70  70\n",
      "  70  70  71  71  71  71  81  82  83  54  54  55  56  78  78  78  78  78\n",
      "  79  80  80  96  96  97  97  97  98  98  98  98  98  99 100 101  24  25\n",
      "  26  42  43  44  44 120 120 121 121 121 122 102 102 102 102 103 103 103\n",
      " 103 104 104 104  66  66  67  68 132 132 132 132 133 133 134 134  24  24\n",
      "  25  25  26  78  78  78  78  79  79  79  79  79  80  80  80  66  67  68\n",
      " 111 112 113  51  51  52  52  52  52  52  52  52  52  52  53  24  25  26\n",
      " 123 124 125 125 125 125 111 111 112 112 112 112 112 113 113 113 113 113\n",
      " 120 120 121 121 121 121 121 121 122 111 111 111 111 112 112 112 113 113\n",
      " 113 113  66  67  68  68  57  57  57  58  58  58  58  58  59  59  27  27\n",
      "  27  28  28  28  29  29  29  99 100 100 100 101 129 129 129 129 130 130\n",
      " 130 131 131 131 131 131  24  25  25  25  25  25  25  25  26  87  88  88\n",
      "  88  88  89  66  67  67  67  67  67  68  84  84  85  85  85  86  84  85\n",
      "  86  86  90  90  90  91  91  91  91  92 123 123 124 124 125 125 125  48\n",
      "  48  49  49  49  49  50  50  81  81  81  82  82  82  83  36  37  38  38\n",
      "  51  51  52  52  52  52  52  52  52  52  53  21  22  22  22  23  84  84\n",
      "  85  86  42  43  44  42  43  44  44  44  66  67  68 102 102 102 103 103\n",
      " 103 103 103 103 103 104 104 104  48  48  48  49  49  49  49  50  81  81\n",
      "  81  81  82  82  83  36  37  37  38  51  51  51  52  52  52  52  52  52\n",
      "  52  52  53  53   0   0   0   0   1   1   1   1   2   2   2   2 108 108\n",
      " 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108 108\n",
      " 108 108 108 108 108 109 109 109 109 110]\n"
     ]
    }
   ],
   "source": [
    "dev = wsj.dev\n",
    "print(dev[0].shape)\n",
    "print(dev[1].shape)\n",
    "print(dev[0][3].shape)\n",
    "print(dev[0][0])\n",
    "print(dev[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = wsj.train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_HW1(Dataset):\n",
    "    \n",
    "    def __init__(self, loader):\n",
    " \n",
    "        self.x = np.concatenate(loader[0] if len(loader) == 2 else loader)\n",
    "        self.y = np.concatenate(loader[1]) if (len(loader) == 2 and loader[1] is not None) else None\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = torch.from_numpy(self.x[idx])\n",
    "        \n",
    "        if self.y is not None:\n",
    "            label = torch.from_numpy(np.array(self.y[idx]))\n",
    "            return data, label\n",
    "          \n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartDataset_HW1(Dataset):\n",
    "    \n",
    "    def __init__(self, loader, k = 1):\n",
    " \n",
    "        self.x = np.concatenate(loader[0] if len(loader) == 2 else loader)\n",
    "        self.y = np.concatenate(loader[1]) if (len(loader) == 2 and loader[1] is not None) else None\n",
    "         \n",
    "        self.k = k       \n",
    "        self.len = len(self.x)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "      \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        if (idx < k):\n",
    "            x_slice = np.pad(self.x[:idx + k + 1], ((k - idx, 0), (0, 0)), 'constant')\n",
    "        elif (idx + k >= self.len):\n",
    "            x_slice = np.pad(self.x[idx - k:], ((0, k - (self.len - 1 - idx)), (0, 0)), 'constant')\n",
    "        else:\n",
    "            x_slice = self.x[idx - k : idx + k + 1]\n",
    "        \n",
    "        data = torch.from_numpy(x_slice.flatten())\n",
    "        \n",
    "        if self.y is not None:\n",
    "            label = torch.from_numpy(np.array(self.y[idx]))\n",
    "            return data, label\n",
    "          \n",
    "        else:\n",
    "            return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_ds = SmartDataset_HW1(dev, k)\n",
    "val_loader = DataLoader(val_ds, shuffle=True, batch_size=bs, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = SmartDataset_HW1(train, k)\n",
    "train_loader = DataLoader(train_ds, shuffle=True, batch_size=bs, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_MLP(nn.Module):\n",
    "    def __init__(self, k = 1):\n",
    "        super(Simple_MLP, self).__init__()\n",
    "        \n",
    "        self.k = k\n",
    "        \n",
    "        self.fc1 = nn.Linear((2*k + 1) * 40, 4096)\n",
    "        self.bnorm1 = nn.BatchNorm1d(4096)\n",
    "        #self.dp1 = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(4096, 2048)\n",
    "        self.bnorm2 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        self.fc3 = nn.Linear(2048, 2048)\n",
    "        self.bnorm3 = nn.BatchNorm1d(2048)\n",
    "\n",
    "        self.fc4 = nn.Linear(2048, 2048)\n",
    "        self.bnorm4 = nn.BatchNorm1d(2048)\n",
    "        \n",
    "        #self.dp2 = nn.Dropout(p=0.1)\n",
    "        self.fc5 = nn.Linear(2048, 138)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, (2 * self.k + 1) * 40) # Flatten the input\n",
    "        x = self.bnorm1(F.relu(self.fc1(x)))\n",
    "        #x = self.dp1(x)\n",
    "        x = self.bnorm2(F.relu(self.fc2(x)))\n",
    "        #x = self.dp2(x)\n",
    "        x = self.bnorm3(F.relu(self.fc3(x)))\n",
    "        x = self.bnorm4(F.relu(self.fc4(x)))\n",
    "        x = F.log_softmax(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_xavier(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        fan_in = m.weight.size()[1]\n",
    "        fan_out = m.weight.size()[0]\n",
    "        std = np.sqrt(2.0 / (fan_in + fan_out))\n",
    "        m.weight.data.normal_(0,std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple_MLP(\n",
      "  (fc1): Linear(in_features=2440, out_features=4096, bias=True)\n",
      "  (bnorm1): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=2048, bias=True)\n",
      "  (bnorm2): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (bnorm3): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=2048, out_features=2048, bias=True)\n",
      "  (bnorm4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc5): Linear(in_features=2048, out_features=138, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Simple_MLP(k)\n",
    "model.apply(init_xavier)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    model.cuda()\n",
    "\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):   \n",
    "        optimizer.zero_grad()   \n",
    "        data = data.cuda()\n",
    "        target = target.long().cuda()\n",
    "\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    running_loss /= len(train_loader)\n",
    "    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n",
    "    return running_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, val_loader, criterion):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "\n",
    "        running_loss = 0.0\n",
    "        total_predictions = 0.0\n",
    "        correct_predictions = 0.0\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(val_loader):   \n",
    "            data = data.cuda()\n",
    "            target = target.long().cuda()\n",
    "\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_predictions += target.size(0)\n",
    "            correct_predictions += (predicted == target).sum().item()\n",
    "\n",
    "            loss = criterion(outputs, target).detach()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        running_loss /= len(val_loader)\n",
    "        acc = (correct_predictions/total_predictions)*100.0\n",
    "        print('Validation Loss: ', running_loss)\n",
    "        print('Validation Accuracy: ', acc, '%')\n",
    "        return running_loss, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss:  1.5029252905952308 Time:  507.03837609291077 s\n",
      "Validation Loss:  1.3091304995780326\n",
      "Validation Accuracy:  63.8463814108598 %\n",
      "====================\n"
     ]
    }
   ],
   "source": [
    "for i in range(n_epochs):\n",
    "    tl = train_epoch(model, train_loader, criterion, optimizer)\n",
    "    vl, va = test_model(model, val_loader, criterion)\n",
    "    \n",
    "    train_loss.append(tl)\n",
    "    val_loss.append(vl)\n",
    "    val_acc.append(va)\n",
    "    print('='*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Validation Loss')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.plot(val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = wsj.test\n",
    "test_ds = SmartDataset_HW1(test, k)\n",
    "test_loader = DataLoader(test_ds, shuffle=False, batch_size=bs, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        model.cuda()\n",
    "\n",
    "        preds = []\n",
    "        for batch_idx, data in enumerate(test_loader):   \n",
    "            data = data.cuda()\n",
    "            outputs = model(data)\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            preds.append(predicted.cpu().numpy())\n",
    "\n",
    "        return np.concatenate(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(path + \"/subm1.csv\", header = ['label'], index_label='id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
